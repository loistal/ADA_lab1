{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# IS-Academia Analysis\n",
    "This is a data analysis of the IS-Academia data accessible by anyone, without authentication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal** : \n",
    "* Find out how much time do EPFL's students in Computer Science need to get their Bachelor. \n",
    "* Do a similar analysis for the Master's degree. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The challenge before analysing the data is to extract this data from the IS-Academia website. By looking at this [page](http://isa.epfl.ch/imoniteur_ISAP/%21gedpublicreports.htm?ww_i_reportmodel=133685247), we can extract information about the names and different values of the HTML `<input>` fields using *Beautiful Soup*. Then we will be able to generate a valid request to get the wanted data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing the requests using Postman\n",
    "\n",
    "With the *Postman interceptor*, we can intercept requests when submitting the form we are interested in. \n",
    "A valid request URL looks like this : \n",
    "\n",
    "`http://isa.epfl.ch/imoniteur_ISAP/!GEDPUBLICREPORTS.filter?ww_b_list=1&ww_i_reportmodel=133685247&ww_c_langue=&ww_i_reportModelXsl=133685270&zz_x_UNITE_ACAD=Informatique&ww_x_UNITE_ACAD=249847&zz_x_PERIODE_ACAD=2016-2017&ww_x_PERIODE_ACAD=355925344&zz_x_PERIODE_PEDAGO=Bachelor+semestre+1&ww_x_PERIODE_PEDAGO=249108&zz_x_HIVERETE=Semestre+d%27automne&ww_x_HIVERETE=2936286&dummy=ok`\n",
    "\n",
    "You can see there are redundant information, for example : `zz_x_UNITE_ACAD=Informatique` and `ww_x_UNITE_ACAD=249847`. You can imagine that getting rid of one of them can still work. It's actually the case, you can get rid of all the `zz_x_*` parameters. With a closer analysis, you can see that the `ww_x_*` parameters correspond to the actual values in the HTML dropdowns.\n",
    "\n",
    "So this is also a valid request :\n",
    "\n",
    "`http://isa.epfl.ch/imoniteur_ISAP/!GEDPUBLICREPORTS.filter?ww_b_list=1&ww_i_reportmodel=133685247&ww_c_langue=&ww_i_reportModelXsl=133685270&ww_x_UNITE_ACAD=249847&ww_x_PERIODE_ACAD=355925344&ww_x_PERIODE_PEDAGO=249108&ww_x_HIVERETE=2936286&dummy=ok`, much more simpler and shorter. \n",
    "\n",
    "Then the URL of the empty form page is given by \n",
    "`http://isa.epfl.ch/imoniteur_ISAP/!GEDPUBLICREPORTS.filter?ww_i_reportmodel=133685247`\n",
    "which is a little bit different from the IS-Academia link above, because the page is using frames and their HTML code are not included in the base page. \n",
    "\n",
    "From here, you can already guess the form paramters we are going to use, but the goal is to extract them and not hardcode them. At least now we have the request base and format.\n",
    "\n",
    "IS-Academia generates a link, on which you can click to get the actual data. By inspecting the requests, we can see that the URL required to get the data contains all the parameters, and additionaly a parameter contained in the HTML source of the link, we will need to extract it to get the final data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the parameters using Beautiful Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The principle used to extract the parameters here is basically to use the text description provided in the HTML page, and then play with the DOM to extract the wanted values. Parameters are returned as a dictionnary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import Requests and Beautiful Soup\n",
    "import requests as rq\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the IS-Academia page\n",
    "empty_form_url = 'http://isa.epfl.ch/imoniteur_ISAP/!GEDPUBLICREPORTS.filter?ww_i_reportmodel=133685247'\n",
    "\n",
    "# Get the page by doing a HTTP request\n",
    "empty_form = rq.get(empty_form_url)\n",
    "if empty_form.status_code != rq.codes.ok:\n",
    "    print(\"--> Error, I'm gonna crash... <--\")\n",
    "\n",
    "# Get the soup out of it\n",
    "form_soup = BeautifulSoup(empty_form.text, 'html.parser')\n",
    "\n",
    "def get_parameters(format, filters):\n",
    "    \"\"\"\n",
    "    Returns a dictionnary containing the paramters\n",
    "    \n",
    "    format : 'xls' or 'html', selects the data format\n",
    "    filters : dictionnary containing the values you want to select in the dropdowns\n",
    "    \"\"\"\n",
    "    # Parameters dictionnary\n",
    "    parameters = {}\n",
    "    \n",
    "    # First, get the checkbox parameters\n",
    "    checkbox = form_soup.find(text = format).parent\n",
    "    parameters[checkbox['name']] = checkbox['value']\n",
    "    \n",
    "    # Then get the dropdown parameters\n",
    "    for f in filters:\n",
    "        html_option = form_soup.find(text = f).parent\n",
    "        param_name = html_option.parent['name']\n",
    "        param_value = html_option['value']\n",
    "        parameters[param_name] = param_value\n",
    "        \n",
    "    # Don't forget the hidden fields\n",
    "    hidden = form_soup.find_all('input', type='hidden')\n",
    "    \n",
    "    # Ignore the zz_* fields, they are useless for what we want to do, \n",
    "    # also ignore ww_i_reportmodel, already contained in the base URL\n",
    "    for field in hidden:\n",
    "        if not (field['name'].startswith('z') or field['name'] == 'ww_i_reportmodel'):\n",
    "            parameters[field['name']] = field['value']\n",
    "            \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the actual data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the parameters, we are able to get the desired link to the data, a little bit of work on this page and we can \"simulate\" a click on this link by generating the good request. The data is then saved in a file, and the `get_data` function returns its path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_data(parameters, format, filename):\n",
    "    \"\"\"\n",
    "    Returns a file path with the data\n",
    "    \n",
    "    url : url of the file to download\n",
    "    parameters : parameters of the request\n",
    "    format : 'html' or 'xls\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the webpage with the data link\n",
    "    link_page = rq.get(empty_form_url, parameters)\n",
    "    if link_page.status_code != rq.codes.ok:\n",
    "        print(\"--> Error, I'm gonna crash... <--\")\n",
    "    \n",
    "    # Get the soup out of it\n",
    "    link_soup = BeautifulSoup(link_page.text, 'html.parser')\n",
    "\n",
    "    # The interesting link is the second of the page, and the parameters in this context : onlick:\"...'name=value'\".\n",
    "    link_param = link_soup.find_all('a')[1]['onclick'].split('\\'')[1].split('=')\n",
    "    parameters[link_param[0]] = link_param[1]\n",
    "\n",
    "    # The request needs a capitalized xls\n",
    "    format = 'XLS' if format == 'xls' else format\n",
    "    \n",
    "    url = empty_form_url.replace('filter', format)\n",
    "        \n",
    "    # Thanks to http://stackoverflow.com/questions/16694907/how-to-download-large-file-in-python-with-requests-py\n",
    "    r = rq.get(url, parameters)\n",
    "    filename = filename + '.%s' % format\n",
    "    \n",
    "    with open(filename, 'wb') as f:\n",
    "        for chunk in r.iter_content(chunk_size=1024): \n",
    "            if chunk: # Filter out keep-alive new chunks\n",
    "                f.write(chunk) \n",
    "    return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to fetch the desired data and save it in a file : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "format = 'xls'\n",
    "filters = ['Informatique', 'Bachelor semestre 1', '2016-2017', 'Semestre d\\'automne']\n",
    "filename = 'test'\n",
    "\n",
    "# Get the request parameters\n",
    "params = get_parameters(format, filters)\n",
    "    \n",
    "get_data(params, format, filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
